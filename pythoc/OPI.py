# -*- coding: utf-8 -*-
"""corona_influenza_no_last_update.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1du_9yzm4EBVYQsmy09k18f384kXYqWo1
"""

#from google.colab import drive
#drive.mount('/content/drive')

!pip install pyswarms==0.1.9

import torch
import numpy
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df =  pd.read_csv('/data/combine_corona_infulenza1.csv',na_values="-N/A-")
df = df.replace({"-N/A-":np.nan}, inplace=False)
df = df.replace({"Unknown":np.nan}, inplace=False)
c=df.columns
#for i,j in zip(list((df.head(0))),np.arange(0,77,1)):
#  print(f'{c[j]}   null values is:  ',df[f'{i}'].isnull().sum())

df = df.fillna(df.mode().iloc[0])

#for i,j in zip(list((df.head(0))),np.arange(0,77,1)):
#  print(f'{c[j]}   null values is:  ',df[f'{i}'].isnull().sum())

fatigue =[]
d = df['Fatigue '].value_counts()
for x in  d.index:
  fatigue.append(x)

for x in fatigue:
  if x=='Yes' :
    i=np.where(df['Fatigue ']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([1])
  if x=='fatigue(Yes)"' :
    i=np.where(df['Fatigue ']== 'fatigue(Yes)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([1])    
  if x=='fatigue(Yes)' :
    i=np.where(df['Fatigue ']== 'fatigue(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([1]) 
  if x=='No' :
    i=np.where(df['Fatigue ']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([0])
  if x=='fatigue(No)' :
    i=np.where(df['Fatigue ']== 'fatigue(No)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([0])
  if x=='fatigue(Moderate)' :
    i=np.where(df['Fatigue ']== 'fatigue(Moderate)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([2])
  if x=='fatigue(Mild)' :
    i=np.where(df['Fatigue ']== 'fatigue(Mild)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([2])  
  if x=='fatigue(Mild)"' :
    i=np.where(df['Fatigue ']== 'fatigue(Mild)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([2])  
  if x=='fatigue(Moderate)"' :
    i=np.where(df['Fatigue ']== 'fatigue(Moderate)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([2]) 
  if x=='fatigue(Severe)' :
    i=np.where(df['Fatigue ']== 'fatigue(Severe)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([3])
  if x=='fatigue(4)' :
    i=np.where(df['Fatigue ']== 'fatigue(4)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([2]) 
  if x=='fatigue(EH)"' :
    i=np.where(df['Fatigue ']== 'fatigue(EH)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([2])        
  if x=='fatigue(None)' :
    i=np.where(df['Fatigue ']== 'fatigue(None)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,11]=np.array([-1])
  else:
    for x in df['Fatigue '].value_counts().index[5:]:
      i=np.where(df['Fatigue '] == x)
      i1=[[x for x in j]for j in i ]
      df.iloc[i1,11]=np.array([-1])

Gastrointestinal =[]
d = df['Gastrointestinal '].value_counts()
for x in  d.index:
  Gastrointestinal.append(x)

for x in Gastrointestinal:
  if x=='Yes' :
    i=np.where(df['Gastrointestinal ']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,12]=np.array([1])
  if x=='diarrhea(Yes)' :
    i=np.where(df['Gastrointestinal ']== 'diarrhea(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,12]=np.array([1]) 
  else :
    i=np.where(df['Gastrointestinal ']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,12]=np.array([0])

d =df['Fever'].value_counts()
fever = []
for x in  d.index:
  fever.append(x)

for x in fever:
  if x=='Yes' :
    i=np.where(df['Fever']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([1])
  elif x=='fever(Yes)' :
    i=np.where(df['Fever']== 'fever(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([1]) 
  elif x=='fever(TRUE)"' :
    i=np.where(df['Fever']== 'fever(TRUE)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([1])
  elif x=='fever(Yes)"' :
    i=np.where(df['Fever']== 'fever(Yes)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([1])
  elif x=='No' :
    i=np.where(df['Fever']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([0])
  elif x=='fever(No)' :
    i=np.where(df['Fever']== 'fever(No)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([0]) 
  elif x=='fever(None)' :
    i=np.where(df['Fever']== 'fever(None)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([-1])
  elif x=='fever(None)"' :
    i=np.where(df['Fever']== 'fever(None)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([-1])    
  elif x=='temperature(101.84)' :
    i=np.where(df['Fever']== 'temperature(101.84)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2]) 
  elif x=='high_temp_home(101.7)' :
    i=np.where(df['Fever']== 'high_temp_home(101.7)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2]) 
  elif x=='temperature(100.2)' :
    i=np.where(df['Fever']== 'temperature(100.2)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2])
  elif x=='temperature(100.1)' :
    i=np.where(df['Fever']== 'temperature(100.1)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2])    
  elif x=='temperature(101.66)' :
    i=np.where(df['Fever']== 'temperature(101.66)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2]) 
  elif x=='high_temp_home(101.12)' :
    i=np.where(df['Fever']== 'high_temp_home(101.12)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2]) 
  elif x=='temperature(101.66)' :
    i=np.where(df['Fever']== 'temperature(101.66)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2]) 
  elif x=='temperature(100.8)"' :
    i=np.where(df['Fever']== 'temperature(100.8)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2])   
  elif x=='fever(Mild)' :
    i=np.where(df['Fever']== 'fever(Mild)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2])
  elif x=='fever(Moderate)' :
    i=np.where(df['Fever']== 'fever(Moderate)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2])
  elif x=='temperature(100.8)"' :
    i=np.where(df['Fever']== 'temperature(100.8)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,1]=np.array([2]) 
  else:
    for x in df['Fever'].value_counts().index[4:10]:
      i=np.where(df['Fever'] == x)
      i1=[[x for x in j]for j in i ]
      df.iloc[i1,1]=np.array([4]) 
    for x in df['Fever'].value_counts().index[4:-6]:
      i=np.where(df['Fever'] == x)
      i1=[[x for x in j]for j in i ]
      df.iloc[i1,1]=np.array([2])  
    for x in df['Fever'].value_counts().index[4:]:
      i=np.where(df['Fever'] == x)
      i1=[[x for x in j]for j in i ]
      df.iloc[i1,1]=np.array([4])

Sore_throat = ['Yes','No','throat(Yes)','throat(No)','throat(Unk)']

for x in Sore_throat:
  if x=='Yes' :
    i=np.where(df['Sore throat']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,3]=np.array([1])
  if x=='throat(Yes)' :
    i=np.where(df['Sore throat']== 'throat(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,3]=np.array([1]) 
  if x=='No' :
    i=np.where(df['Sore throat']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,3]=np.array([0])
  if x=='throat(No)' :
    i=np.where(df['Sore throat']== 'throat(No)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,3]=np.array([0])
  if x=='throat(Unk)' :
    i=np.where(df['Sore throat']== 'throat(Unk)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,3]=np.array([-1])

Running_Nose = ['Yes','No','running nose(Yes)','running nose(No)','running nose(Mild)',
                'running nose(None)','running nose(Unk)','running nose(4)']

for x in Running_Nose:
  if x=='Yes' :
    i=np.where(df['Running Nose']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,4]=np.array([1])
  if x=='running nose(Yes)' :
    i=np.where(df['Running Nose']== 'running nose(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,4]=np.array([1]) 
  if x=='No' :
    i=np.where(df['Running Nose']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,4]=np.array([0])
  if x=='running nose(No)' :
    i=np.where(df['Running Nose']== 'running nose(No)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,4]=np.array([0])
  if x=='running nose(None)' :
    i=np.where(df['Running Nose']== 'running nose(None)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,4]=np.array([-1])  
  if x=='running nose(Unk)' :
    i=np.where(df['Running Nose'] == 'running nose(Unk)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,4]=np.array([-1]) 
  if x=='running nose(4)' :
    i=np.where(df['Running Nose'] == 'running nose(4)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,4]=np.array([3])  
  if x=='running nose(Mild)' :
    i=np.where(df['Running Nose'] == 'running nose(Mild)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,4]=np.array([2])

Hyper_Tension = ['No','Yes','chills(Yes)','chills(No)','chills(Unk)','nausea(Yes)']

for x in Hyper_Tension:
  if x=='Yes' :
    i=np.where(df['Hyper Tension']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,10]=np.array([1])
  if x=='No' :
    i=np.where(df['Hyper Tension']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,10]=np.array([0]) 
  if x=='chills(Yes)' :
    i=np.where(df['Hyper Tension']== 'chills(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,10]=np.array([1])
  if x=='chills(No)' :
    i=np.where(df['Hyper Tension']== 'chills(No)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,10]=np.array([0])
  if x=='chills(Unk)' :
    i=np.where(df['Hyper Tension']== 'chills(Unk)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,10]=np.array([-1])  
  if x=='nausea(Yes)' :
    i=np.where(df['Hyper Tension']== 'nausea(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,10]=np.array([-1])

Headache = ['Yes','No','aches(Yes)','headache(None)','headache(Mild)','headache(Severe)','headache(Moderate)',
            'headache(4)','aches(No)','aches(Unk)','headache(Yes)','headache(Not reported)',
            'headache(TRUE)','throat(ukn)']

for x in Headache:
  if x=='Yes' :
    i=np.where(df['Headache']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([1])
#for x in Dry_Cough:    
  if x=='aches(Yes)' :
    i=np.where(df['Headache']== 'aches(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([1])
#for x in Dry_Cough:    
  if x=='headache(TRUE)' :
    i=np.where(df['Headache']== 'headache(TRUE)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([1])
  if x=='headache(Yes)' :
    i=np.where(df['Headache']== 'headache(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([1])    
#for x in Dry_Cough:    
  if x=='headache(Moderate)' :
    i=np.where(df['Headache']== 'headache(Moderate)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([2])
  if x=='headache(Mild)' :
    i=np.where(df['Headache']== 'headache(Mild)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([2])  
#for x in Dry_Cough:    
  if x=='headache(Severe)' :
    i=np.where(df['Headache']== 'headache(Severe)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([3])    
#for x in Dry_Cough:    
  if x=='headache(4)' :
    i=np.where(df['Headache']== 'headache(4)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([3])      
#for x in Dry_Cough:    
  if x=='No' :
    i=np.where(df['Headache']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([0])
#for x in Dry_Cough:      
  if x=='aches(No)' :
    i=np.where(df['Headache']== 'aches(No)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([0])
#for x in Dry_Cough:    
  if x=='headache(None)':
    i=np.where(df['Headache']== 'headache(None)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([-1]) 
#for x in Dry_Cough:    
  if x=='aches(Unk)' :
    i=np.where(df['Headache']== 'aches(Unk)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([-1])  
  if x=='headache(Not reported)' :
    i=np.where(df['Headache']== 'headache(Not reported)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([-1])       
  if x=='throat(ukn)' :
    i=np.where(df['Headache']== 'throat(ukn)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,7]=np.array([-1])

Dry_Cough = ['Yes','No','cough(Yes)','cough(None)','cough(Mild)','cough(4)','cough(TRUE)',
             'cough(Moderate)','cough(No)','cough(Not reported)','chills(Yes)']

for x in Dry_Cough:
  if x=='Yes' :
    i=np.where(df['Dry Cough']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([1])
#for x in Dry_Cough:    
  if x=='cough(Yes)' :
    i=np.where(df['Dry Cough']== 'cough(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([0.8])
#for x in Dry_Cough:    
  if x=='cough(TRUE)' :
    i=np.where(df['Dry Cough']== 'cough(TRUE)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([0.8])
#for x in Dry_Cough:    
  if x=='cough(Mild)' :
    i=np.where(df['Dry Cough']== 'cough(Mild)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([2])
#for x in Dry_Cough:    
  if x=='cough(Moderate)' :
    i=np.where(df['Dry Cough']== 'cough(Moderate)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([2])    
#for x in Dry_Cough:    
  if x=='cough(4)' :
    i=np.where(df['Dry Cough']== 'cough(4)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([3])      
#for x in Dry_Cough:    
  if x=='No' :
    i=np.where(df['Dry Cough']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([0])
#for x in Dry_Cough:      
  if x=='cough(No)' :
    i=np.where(df['Dry Cough']== 'cough(No)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([0])
#for x in Dry_Cough:    
  if x=='cough(None)' :
    i=np.where(df['Dry Cough']== 'cough(None)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([-1]) 
#for x in Dry_Cough:    
  if x=='cough(Not reported)' :
    i=np.where(df['Dry Cough']== 'cough(Not reported)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([-1])  
  if x=='chills(Yes)' :
    i=np.where(df['Dry Cough']== 'chills(Yes)')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,2]=np.array([-1])

Breathing_Problem = ['Yes','No','dyspnea(No)"','dyspnea(Yes)"','dyspnea(Unk)"']

for x in Breathing_Problem:
  if x=='Yes' :
    i=np.where(df['Breathing Problem']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,0]=np.array([1])
for x in Breathing_Problem:    
  if x=='dyspnea(Yes)"' :
    i=np.where(df['Breathing Problem']== 'dyspnea(Yes)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,0]=np.array([1])
for x in Breathing_Problem:    
  if x=='No' :
    i=np.where(df['Breathing Problem']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,0]=np.array([0])
for x in Breathing_Problem:      
  if x=='dyspnea(No)"' :
    i=np.where(df['Breathing Problem']== 'dyspnea(No)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,0]=np.array([0])
for x in Breathing_Problem:    
  if x=='dyspnea(Unk)"' :
    i=np.where(df['Breathing Problem']== 'dyspnea(Unk)"')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,0]=np.array([-1])

target=df['COVID-19-H1N1-NO']
target_encoding=[]
for x in target:
  if x=='No':
    target_encoding.append(np.array([0]))
  elif  x=='H1N1':
    target_encoding.append(np.array([1])) 
  else:
    target_encoding.append(np.array([2]))

print(target_encoding[-11:-1])
target_encoding1=np.array(target_encoding)
target_encoding=np.array(target_encoding)

COVID =['No','H1N1','Yes']
for x in COVID :    
  if x=='No' :
    i=np.where(df['COVID-19-H1N1-NO']== 'No')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,-1]=np.array([0])      
  if x=='H1N1' :
    i=np.where(df['COVID-19-H1N1-NO']== 'H1N1')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,-1]=np.array([1])   
  if x=='Yes' :
    i=np.where(df['COVID-19-H1N1-NO']== 'Yes')
    i1=[[x for x in j]for j in i ]
    df.iloc[i1,-1]=np.array([2])

one_hot_encoded = pd.get_dummies(df[['Asthma', 'Chronic Lung Disease', 
       'Heart Disease', 'Diabetes', 
       'Abroad travel', 'Contact with COVID Patient',
       'Attended Large Gathering', 'Visited Public Exposed Places',
       'Family working in Public Exposed Places', 'Wearing Masks',
       'Sanitization from Market']], prefix_sep='_', drop_first=False)

#one_hot_encoded

arithmical_relation = df[['Breathing Problem','Fever','Dry Cough','Sore throat',
                          'Sore throat','Headache','Hyper Tension','Fatigue ', 'Gastrointestinal ' ,'COVID-19-H1N1-NO']]

#arithmical_relation

data_num_new = pd.concat([arithmical_relation,one_hot_encoded ],axis=1, sort=False)
data_num_new1 = pd.DataFrame(data_num_new)

data_num_new1.shape

import seaborn as sns

!pip install category_encoders

import category_encoders as ce
c5=one_hot_encoded.columns
encoder5 = ce.TargetEncoder(cols=[x for x in c5])

target_encoder_all2= encoder5.fit_transform(one_hot_encoded,target_encoding)

column4=['Breathing Problem','Fever','Dry Cough',
        'Sore throat','Headache','Hyper Tension','Fatigue ',
         'Gastrointestinal ' ,'COVID-19-H1N1-NO']
encoder4 = ce.TargetEncoder(cols=[x for x in column4])
target_encoder_all= encoder4.fit_transform(df[['Breathing Problem','Fever','Dry Cough',
        'Sore throat','Headache','Hyper Tension','Fatigue ',
         'Gastrointestinal ' ,'COVID-19-H1N1-NO']],target_encoding)

df2 =df

from sklearn.preprocessing import LabelEncoder
var_mod = ['Asthma', 'Chronic Lung Disease', 
       'Heart Disease', 'Diabetes', 
       'Abroad travel', 'Contact with COVID Patient',
       'Attended Large Gathering', 'Visited Public Exposed Places',
       'Family working in Public Exposed Places', 'Wearing Masks',
       'Sanitization from Market']
le = LabelEncoder()
for i in var_mod:
    df2[i] = le.fit_transform(df2[i])
df2 = df2.astype('int64')
df2.dtypes

df2.shape

data_num_new = pd.concat([target_encoder_all,target_encoder_all2],axis=1, sort=False)
data_num_new1 = pd.DataFrame(data_num_new)

data_num_new1 =df2

data_num_new1 = data_num_new1.drop('Wearing Masks', axis=1)
data_num_new1 = data_num_new1.drop('Sanitization from Market', axis=1)

data_num_new1.rename(columns={'COVID-19-H1N1-NO':'Neither COVID-19 Nor H1N1'}, inplace=True)

#data_num_new1.rename(index = {'COVID-19-H1N1-NO':'Neither COVID-19 Nor H1N1'},
#                                 inplace = True)

import matplotlib.pyplot as plt
import matplotlib 
font = {'family' : 'normal',
        'size'   : 10}
SMALL_SIZE = 18     
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)
matplotlib.rc('font', **font) 
plt.figure(figsize=(11, 11))
#plt.title('correlation between each features')
corr =data_num_new1.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(5, 220, n=50),
    square=True
)

ax.set_xticklabels(    
    ax.get_xticklabels(),
    rotation=40,
    horizontalalignment='right',
    
    
);

from sklearn.feature_selection import VarianceThreshold

constant_filter = VarianceThreshold(threshold=0.12)

constant_filter.fit(data_num_new1)

constant_columns = [column for column in data_num_new1.columns
                    if column not in data_num_new1.columns[constant_filter.get_support()]]

print(constant_columns)

data_num_new1.shape

data_num_new1.head()

#data_num_new1 = data_num_new1.drop('Wearing Masks', axis=1)
#data_num_new1 = data_num_new1.drop('Sanitization from Market', axis=1)
data_num_new1 = data_num_new1.drop('Neither COVID-19 Nor H1N1', axis=1)

data_num_new1.shape

X = data_num_new1
y = target_encoding1
y =y.ravel()
print(X.shape,y.shape)



from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=21)

model = XGBClassifier()
model.fit(X_train, y_train)
y_pred_train = model.predict(X_train)
y_pred = model.predict(X_test)
accuracy_train = accuracy_score(y_train, y_pred_train)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
print("Accuracy train: %.2f%%" % (accuracy_train * 100.0))

from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt

conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)
print('Confusion matrix:\n', conf_mat)
labels = ['No','INFLUENZA','Cororna']
fig = plt.figure(figsize=(10, 5))
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Reds)
fig.colorbar(cax)
fig.set_size_inches(10, 5)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('actual')
plt.show()



#from imblearn.under_sampling import RandomUnderSampler

#US = RandomUnderSampler(sampling_strategy='auto', random_state=0)
#X_rus, y_rus, id_rus = US.fit_resample(X, y)
#print(X_rus.shape, y_rus.shape)

from imblearn.over_sampling import RandomOverSampler

OS = RandomOverSampler(sampling_strategy='auto', random_state=0)
X_ros, y_ros = OS.fit_resample(X, y)
print(X_ros.shape, y_ros.shape)

from imblearn.over_sampling import SMOTE
smote = SMOTE(sampling_strategy='minority',random_state=0)
X_sm, y_sm = smote.fit_resample(X, y)
print(X_sm.shape, y_sm.shape )

import time
import numpy as np
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn import datasets
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
import time
import numpy as np
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn import datasets
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier

log_clf = LogisticRegression(penalty='l2',max_iter=300,C=1,solver='lbfgs',l1_ratio=None)
XGB_clf = XGBClassifier()

import pandas as pd
import numpy as np
from scipy import interp

from  sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import LabelBinarizer

def class_report(y_true, y_pred, y_score=None, average='micro'):
  if y_true.shape != y_pred.shape:
    print("Error! y_true %s is not the same shape as y_pred %s" % (
          y_true.shape,
          y_pred.shape)
        )
    return

  lb = LabelBinarizer()

  if len(y_true.shape) == 1:
        lb.fit(y_true)

  #Value counts of predictions
  labels, cnt = np.unique(y_pred,return_counts=True)
  n_classes = len(labels)
  pred_cnt = pd.Series(cnt, index=labels)

  metrics_summary = precision_recall_fscore_support(y_true=y_true,y_pred=y_pred,labels=labels)

  avg = list(precision_recall_fscore_support(y_true=y_true, y_pred=y_pred,average='weighted'))

  metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']
  class_report_df = pd.DataFrame(list(metrics_summary),index=metrics_sum_index,columns=labels)

  support = class_report_df.loc['support']
  total = support.sum() 
  class_report_df['avg / total'] = avg[:-1] + [total]

  class_report_df = class_report_df.T
  class_report_df['pred'] = pred_cnt
  class_report_df['pred'].iloc[-1] = total

  if not (y_score is None):
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for label_it, label in enumerate(labels):
      fpr[label], tpr[label], _ = roc_curve((y_true == label).astype(int), y_score[:, label_it])
      roc_auc[label] = auc(fpr[label], tpr[label])

    if average == 'micro':
      if n_classes <= 2:
        fpr["avg / total"], tpr["avg / total"], _ = roc_curve(lb.transform(y_true).ravel(), y_score[:, 1].ravel())
      else:
        fpr["avg / total"], tpr["avg / total"], _ = roc_curve(lb.transform(y_true).ravel(), y_score.ravel())

      roc_auc["avg / total"] = auc(fpr["avg / total"],tpr["avg / total"])

    elif average == 'macro':
      # First aggregate all false positive rates
      all_fpr = np.unique(np.concatenate([fpr[i] for i in labels]))
      # Then interpolate all ROC curves at this points
      mean_tpr = np.zeros_like(all_fpr)
      for i in labels:
        mean_tpr += interp(all_fpr, fpr[i], tpr[i])
        # Finally average it and compute AUC
      mean_tpr /= n_classes
      fpr["macro"] = all_fpr
      tpr["macro"] = mean_tpr
      roc_auc["avg / total"] = auc(fpr["macro"], tpr["macro"])

    class_report_df['AUC'] = pd.Series(roc_auc)

  return class_report_df

X_trainsm, X_testsm, y_trainsm, y_testsm = train_test_split(X_sm, y_sm, test_size=0.5, random_state=142)
print(X_trainsm.shape, X_testsm.shape, y_trainsm.shape, y_testsm.shape)

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 1, stop = 41, num = 5)]
# Number of features to consider at every split
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
learning_rate=[0.001,0.01,0.1,0.2,0.3,0.5,0.0001,0.5,0.7]
max_depth.append(None)
# Minimum number of samples required to split a node
# Method of selecting samples for training each tree
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'learning_rate': learning_rate}
print(random_grid)

#model = XGBClassifier()
#rf_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
#rf_random.fit(X_trainsm, y_trainsm)

#rf_random.best_params_

model=XGBClassifier(n_estimators=41,max_depth=100,learning_rate=0.2)

model = XGBClassifier()
model.fit(X_trainsm, y_trainsm)
y_pred_trainsm = model.predict(X_trainsm)
y_predsm = model.predict(X_testsm)
accuracy_trainsm = accuracy_score(y_trainsm, y_pred_trainsm)
accuracysm = accuracy_score(y_testsm, y_predsm)
print("Accuracysm: %.2f%%" % (accuracysm * 100.0))
print("Accuracy trainsm: %.2f%%" % (accuracy_trainsm * 100.0))

from sklearn.metrics import confusion_matrix
cm1 = confusion_matrix(y_testsm,y_predsm)
print('Confusion Matrix : \n', cm1)

total1=sum(sum(cm1))
#####from confusion matrix calculate accuracy
accuracy1=(cm1[0,0]+cm1[1,1]+cm1[2,2])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1]+cm1[0,2])
sensitivity2 = cm1[1,1]/(cm1[1,0]+cm1[1,1]+cm1[1,2])
sensitivity3 = cm1[2,2]/(cm1[2,0]+cm1[2,1]+cm1[2,2])
print('Sensitivity of class NO: ', sensitivity1 )
print('Sensitivity of class H1N1: ', sensitivity2 )
print('Sensitivity of class COVID-19: ', sensitivity3 )
specificity1 = cm1[0,0]/(cm1[0,0]+cm1[1,0]+cm1[2,0])
specificity2 = cm1[1,1]/(cm1[0,1]+cm1[1,1]+cm1[2,1])
specificity3 = cm1[2,2]/(cm1[0,2]+cm1[1,2]+cm1[2,2])
print('Specificity of class NO: ', specificity1)
print('Specificity of class H1N1: ', specificity2)
print('Specificity of class COVID-19: ', specificity3)

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer
cv_results = cross_validate(model,X_trainsm, y_trainsm, cv=5)
sorted(cv_results.keys())

cv_results['test_score']

!pip install scikit-plot

import scikitplot as skplt
import matplotlib.pyplot as plt

probas = model.predict_proba(X_testsm)

y_testsm = y_testsm.reshape(y_testsm.shape[0])
y_predsm = y_predsm.reshape(y_testsm.shape[0])
print(y_testsm.shape,y_predsm.shape)

skplt.metrics.plot_precision_recall(y_testsm,probas)

skplt.metrics.plot_roc_curve(y_testsm,probas,title="ROC Curves with XGBClassifier", cmap='Blues', figsize=[7,7])
plt.show()

skplt.metrics.plot_confusion_matrix(y_testsm,y_predsm, normalize=True, cmap='Blues')
plt.show()

report_with_auc = class_report(
    y_true=y_testsm, 
    y_pred=model.predict(X_testsm), 
    y_score=model.predict_proba(X_testsm))
print("XGBClassifier\n",report_with_auc)

log_clf = LogisticRegression(penalty='l2',max_iter=1000,C=1,solver='lbfgs',l1_ratio=None)
log_clf.fit(X_trainsm, y_trainsm)
y_pred_trainsmlog_clf = log_clf.predict(X_trainsm)
y_predsmlog_clf = log_clf.predict(X_testsm)
accuracy_trainsm = accuracy_score(y_trainsm, y_pred_trainsmlog_clf)
accuracysm = accuracy_score(y_testsm, y_predsmlog_clf)
print("Accuracysm of log_clf: %.2f%%" % (accuracysm * 100.0))
print("Accuracy trainsm of log_clf: %.2f%%" % (accuracy_trainsm * 100.0))

from sklearn.metrics import confusion_matrix
cm1 = confusion_matrix(y_testsm,y_predsmlog_clf)
print('Confusion Matrix : \n', cm1)

total1=sum(sum(cm1))
#####from confusion matrix calculate accuracy
accuracy1=(cm1[0,0]+cm1[1,1]+cm1[2,2])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1]+cm1[0,2])
sensitivity2 = cm1[1,1]/(cm1[1,0]+cm1[1,1]+cm1[1,2])
sensitivity3 = cm1[2,2]/(cm1[2,0]+cm1[2,1]+cm1[2,2])
print('Sensitivity of class NO: ', sensitivity1 )
print('Sensitivity of class H1N1: ', sensitivity2 )
print('Sensitivity of class COVID-19: ', sensitivity3 )
specificity1 = cm1[0,0]/(cm1[0,0]+cm1[1,0]+cm1[2,0])
specificity2 = cm1[1,1]/(cm1[0,1]+cm1[1,1]+cm1[2,1])
specificity3 = cm1[2,2]/(cm1[0,2]+cm1[1,2]+cm1[2,2])
print('Specificity of class NO: ', specificity1)
print('Specificity of class H1N1: ', specificity2)
print('Specificity of class COVID-19: ', specificity3)

probas1 = log_clf.predict_proba(X_testsm)

skplt.metrics.plot_roc_curve(y_testsm,probas1,title="ROC Curves with LogisticRegression", cmap='hot_r', figsize=[7,7])
plt.show()

skplt.metrics.plot_confusion_matrix(y_testsm,y_predsmlog_clf, normalize=True, cmap='hot_r')
plt.show()

report_with_auc = class_report(
    y_true=y_testsm, 
    y_pred=log_clf.predict(X_testsm), 
    y_score=log_clf.predict_proba(X_testsm))
print("LogisticRegression\n",report_with_auc)

RandomForest=RandomForestClassifier(max_depth=80,max_features=3,min_samples_leaf=3,min_samples_split=12,n_estimators=40)
RandomForest.fit(X_trainsm, y_trainsm)
y_pred_trainsm_rnd = RandomForest.predict(X_trainsm)
y_predsm_rnd = RandomForest.predict(X_testsm)
accuracy_trainsm = accuracy_score(y_trainsm, y_pred_trainsm_rnd)
accuracysm = accuracy_score(y_testsm, y_predsm_rnd)
print("Accuracysm of random_forest: %.2f%%" % (accuracysm * 100.0))
print("Accuracy trainsm of random_forest: %.2f%%" % (accuracy_trainsm * 100.0))

from sklearn.metrics import confusion_matrix
cm1 = confusion_matrix(y_predsm_rnd,y_testsm)
print('Confusion Matrix : \n', cm1)

total1=sum(sum(cm1))
#####from confusion matrix calculate accuracy
accuracy1=(cm1[0,0]+cm1[1,1]+cm1[2,2])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1]+cm1[0,2])
sensitivity2 = cm1[1,1]/(cm1[1,0]+cm1[1,1]+cm1[1,2])
sensitivity3 = cm1[2,2]/(cm1[2,0]+cm1[2,1]+cm1[2,2])
print('Sensitivity of class NO: ', sensitivity1 )
print('Sensitivity of class H1N1: ', sensitivity2 )
print('Sensitivity of class COVID-19: ', sensitivity3 )
specificity1 = cm1[0,0]/(cm1[0,0]+cm1[1,0]+cm1[2,0])
specificity2 = cm1[1,1]/(cm1[0,1]+cm1[1,1]+cm1[2,1])
specificity3 = cm1[2,2]/(cm1[0,2]+cm1[1,2]+cm1[2,2])
print('Specificity of class NO: ', specificity1)
print('Specificity of class H1N1: ', specificity2)
print('Specificity of class COVID-19: ', specificity3)

from sklearn.model_selection import cross_validate

cv_results = cross_validate(RandomForest, X_trainsm, y_trainsm, cv=15)

print(cv_results['test_score'])

probas2 = RandomForest.predict_proba(X_testsm)

skplt.metrics.plot_roc_curve(y_testsm,probas1,title="ROC Curves with RandomForestClassifier", cmap='BuGn', figsize=[7,7])
plt.show()



report_with_auc = class_report(
    y_true=y_testsm, 
    y_pred=RandomForest.predict(X_testsm), 
    y_score=RandomForest.predict_proba(X_testsm))
print("RandomForest\n",report_with_auc)

skplt.metrics.plot_confusion_matrix(y_testsm,y_predsm_rnd, normalize=True, cmap='Greens')
plt.show()



from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 1, stop = 100, num = 10)]
# Number of features to consider at every split
max_features = [int(x) for x in np.linspace(start = 1, stop = 100, num = 10)]
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 20, 25, 30, 35, 40]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4, 6, 8, 10, 12]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
print(random_grid)

#rf_random = RandomizedSearchCV(estimator = RandomForest, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=3, random_state=42, n_jobs = -1)
# Fit the random search model
#rf_random.fit(X_sm, y_sm)

#rf_random.best_params_

RandomForest=RandomForestClassifier(bootstrap= False,max_depth= 10,max_features= 1,min_samples_leaf= 1,min_samples_split= 10,n_estimators= 12)

RandomForest.fit(X_trainsm, y_trainsm)
y_pred_trainsm_rnd = RandomForest.predict(X_trainsm)
y_predsm_rnd = RandomForest.predict(X_testsm)
accuracy_trainsm = accuracy_score(y_trainsm, y_pred_trainsm_rnd)
accuracysm = accuracy_score(y_testsm, y_predsm_rnd)
print("Accuracysm of random_forest: %.2f%%" % (accuracysm * 100.0))
print("Accuracy trainsm of random_forest: %.2f%%" % (accuracy_trainsm * 100.0))

from keras.backend import sigmoid
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))

from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation
get_custom_objects().update({'swish': Activation(swish)})

from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt #MatPlotLib usado para desenhar o gráfico criado com o NetworkX

# Import PySwarms
import pyswarms as ps

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline

from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier

classifier = linear_model.LogisticRegression(max_iter=1000)

from sklearn.model_selection import train_test_split
train_set, test_set,target_train,target_test = train_test_split(X, y, test_size=0.3,random_state=21)
print(target_train.shape,target_test.shape)
print(test_set.shape)
print(train_set.shape)

X=pd.DataFrame(X)

def f_per_particle(m, alpha):

  total_features = X.shape[1]
  
  if np.count_nonzero(m) == 0: 
        #if the particle subset is only zeros, get the original set of attributes
    X_subset = X
  else:
    X_subset = X.iloc[:,m==1]
  scores = cross_val_score(classifier, X_subset, y, cv=3)
  P = scores.mean()
  particleScore.append(P)
  particleSize.append(X_subset.shape[1])  
  j = (alpha * (1.0 - P)+ (1.0 - alpha) * (1 - (X_subset.shape[1] / total_features)))
  return j

def f(x, alpha=0.9):
  """Higher-level method to do classification in the
  whole swarm.

  Inputs
    ------
  x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search

  Returns
  -------
  numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
  """
  n_particles = x.shape[0]
  j = [f_per_particle(x[i], alpha) for i in range(n_particles)]
  #print("f j: ", j)
  return np.array(j)

classifier = linear_model.LogisticRegression(max_iter=1000)

from datetime import datetime as dt
import time
from pyswarms.utils.environments import PlotEnvironment
start = dt.now()
print("Started at: ", str(start))
particleScore = list()
particleSize = list()
#mySubsets = list()

# Initialize swarm, arbitrary
options = {'c1': 2, 'c2': 2, 'w':0.3, 'k': 20, 'p':2}

# Call instance of PSO
dimensions = X.shape[1] # dimensions should be the number of features
#optimizer.reset()
#optimizer = ps.single.GlobalBestPSO(n_particles=1, dimensions=dimensions,
#                                    options=options)
optimizer = ps.discrete.BinaryPSO(n_particles=20, dimensions=dimensions, options=options)

# Perform optimization
#cost, pos = optimizer.optimize(f, print_step=1, iters=10, verbose=2)


# Initialize plot environment
plt_env = PlotEnvironment(optimizer, f,10)

# Plot the cost
plt_env.plot_cost(figsize=(8,6));
plt.show()


#print(cost,pos)
end = dt.now()
print("Finished at: ", str(end))
total = end-start
print("Total time spent: ", total)

import matplotlib.pyplot as plt #MatPlotLib usado para desenhar o gráfico criado com o NetworkX

#iterations = list(range(1,len(optimizer.get_cost_history)+1))
plt.figure(figsize=(10,7))
#plt.xlabel('2^i classes')
plt.xlabel('subsetSize')
plt.ylabel('Accuracy')
plt.plot(particleSize, particleScore, 'bo')
#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.grid(True)


#plt.savefig("D:/USP/2018-1/Computação Bioinspirada/Trabalhos/iterationVSerrorRate2.png", format="PNG")


plt.show()

X=pd.DataFrame(X)

from sklearn.model_selection import cross_val_score
# Create two instances of LogisticRegression
#classfier = linear_model.LogisticRegression()
classifier = RandomForestClassifier(bootstrap= False,max_depth= 10,
                                     max_features= 1,min_samples_leaf= 1,
                                     min_samples_split= 10,n_estimators= 12)

rank = list()

fullSet = cross_val_score(classifier, pd.DataFrame(X_sm), y_sm, cv=5)
print("Full set Accuracy: %0.2f (+/- %0.2f)" % (fullSet.mean(), fullSet.std() * 2))
print("----------------------------------------------------------------------------")
bests = optimizer.personal_best_pos #optimizer.get_pos_history
for b in bests:
    # Get the selected features from the final positions

    X_selected_features = X.iloc[:,b==1]  # subset

    # Perform classification and store performance in P
    classifier.fit(X_selected_features, y)
    scores = cross_val_score(classifier, X_selected_features, y, cv=5)
    print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2), b)
    rank.append([scores.mean(), b])
    # Compute performance
  
    subset_performance = (classifier.predict(X_selected_features) == y).mean()
    

    print('Subset performance: %.3f' % (subset_performance))



feature =[]
for x in data_num_new1.columns:
  feature.append(x)

target2=pd.get_dummies(target, prefix_sep='_', drop_first=False)
#target2.head()
target2=np.array(target2)
print(target2.shape)

from imblearn.over_sampling import SMOTE
smote = SMOTE(sampling_strategy='minority',random_state=0)
X_sm, y_sm = smote.fit_resample(X, target2)
print(X_sm.shape, y_sm.shape)

from imblearn.over_sampling import RandomOverSampler

OS = RandomOverSampler(sampling_strategy='auto', random_state=0)
X_ros, y_ros = OS.fit_resample(X, target2)
print(X_ros.shape, y_ros.shape)

df5 = pd.concat([pd.DataFrame(X_sm),pd.DataFrame(X_ros)])
print(df5.shape)
target5 = pd.concat([pd.DataFrame(y_sm),pd.DataFrame(y_ros)])
print(target5.shape)

from sklearn.model_selection import train_test_split
train_set, test_set,target_train,target_test = train_test_split(np.array(df5 ), np.array(target5), test_size=0.3,random_state=0)
print(target_train.shape,target_test.shape)
print(test_set.shape)
print(train_set.shape)

train_set=np.array(train_set)
train_set=train_set.reshape(train_set.shape[0],train_set.shape[1],1)
#target_train = target_train.reshape(60248,1)
train_set1=train_set[0:250]
test_set=np.array(test_set)
test_set=test_set.reshape(test_set.shape[0],test_set.shape[1],1)
#target_test = target_test.reshape(25821,1)
test_set1=test_set[0:250]
target_train1=target_train[0:250]
target_test1=target_test[0:250]
print(train_set.shape,target_train.shape)

train_data=train_set[0:14000]
valid_data=train_set[14000:]
train_target=target_train[0:14000]
valid_target=target_train[14000:]
print(train_data.shape)
print(valid_data.shape)
print(train_target.shape)
print(valid_target.shape)
print(valid_target[5])

from tensorflow.keras import callbacks
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
from tqdm import tqdm
import math
import os
import tensorflow
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.callbacks import *
from tensorflow.keras import backend as K
K.clear_session()
import itertools
import matplotlib.pyplot as plt
import cv2
import matplotlib.cm as cm

from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer,LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
#from keras import layers

from tensorflow import keras

kernel_init = keras.initializers.glorot_uniform()
bias_init = keras.initializers.Constant(value=0)

from tensorflow.keras.layers import concatenate
def inception_module(x,filters_1x1,filters_3x3_reduce,filters_3x3,filters_5x5_reduce,filters_5x5,filters_pool_proj,name=None):
  conv_1x1 = Conv1D(filters_1x1, (1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
  conv_3x3 = Conv1D(filters_3x3_reduce, (1), padding='same', activation='relu',kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
  conv_3x3 = Conv1D(filters_3x3, (3), padding='same', activation='relu',kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)

  conv_5x5 = Conv1D(filters_5x5_reduce, (1), padding='same', activation='relu',kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
  conv_5x5 = Conv1D(filters_5x5, (5), padding='same', activation='relu',kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)

  pool_proj = MaxPooling1D(3, strides=1 , padding='same')(x)
  pool_proj = Conv1D(filters_pool_proj,(1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)

  output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=2, name=name)
    
  return output

#Google net
input_B = tensorflow.keras.layers.Input(shape=[train_data.shape[1],1])
x = Conv1D(32, (7), padding='same', activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_B)
x = MaxPooling1D(3, padding='same', name='max_pool_1_3x3/2')(x)
x = Conv1D(64, 7, padding='same', activation='relu', name='conv_2a_3x3/1')(x)
x = Conv1D(128, 5, padding='same', activation='relu', name='conv_2b_3x3/1')(x)
x = MaxPooling1D(3, padding='same', name='max_pool_2_3x3/2')(x)

x = inception_module(x,
                     filters_1x1=64,
                     filters_3x3_reduce=96,
                     filters_3x3=128,
                     filters_5x5_reduce=16,
                     filters_5x5=32,
                     filters_pool_proj=32,
                     name='inception_3a')
x = inception_module(x,
                     filters_1x1=128,
                     filters_3x3_reduce=128,
                     filters_3x3=192,
                     filters_5x5_reduce=32,
                     filters_5x5=96,
                     filters_pool_proj=64,
                     name='inception_3b')
x = MaxPooling1D(3, padding='same', strides=2, name='max_pool_3_3x3/2')(x)

x = inception_module(x,
                     filters_1x1=192,
                     filters_3x3_reduce=96,
                     filters_3x3=208,
                     filters_5x5_reduce=16,
                     filters_5x5=48,
                     filters_pool_proj=64,
                   name='inception_4a')

x1 = AveragePooling1D((2),padding="same")(x)
x1 = Conv1D(128,(1) ,padding="same", activation='relu',)(x1)
print(x1.shape)
x1 = Flatten()(x1)
x1 = Dense(256, activation='swish')(x1)
x1 = Dropout(0.7)(x1)
x1 = Dense(3, activation='softmax', name='auxilliary_output_1')(x1)

x = inception_module(x,
                     filters_1x1=160,
                     filters_3x3_reduce=112,
                     filters_3x3=224,
                     filters_5x5_reduce=24,
                     filters_5x5=64,
                     filters_pool_proj=64,
                     name='inception_4b')
x = inception_module(x,
                     filters_1x1=128,
                     filters_3x3_reduce=128,
                     filters_3x3=256,
                     filters_5x5_reduce=24,
                     filters_5x5=64,
                     filters_pool_proj=64,
                    name='inception_4c')

x = inception_module(x,
                     filters_1x1=112,
                     filters_3x3_reduce=144,
                     filters_3x3=288,
                     filters_5x5_reduce=32,
                     filters_5x5=64,
                     filters_pool_proj=64,
                     name='inception_4d')

x2 = AveragePooling1D((5),padding='same')(x)
x2 = Conv1D(128,(1) , padding='same', activation='relu',strides=2)(x2)
x2 = Flatten()(x2)
x2 = Dense(256, activation='tanh')(x2)
x2 = Dropout(0.25)(x2)
x2 = Dense(3, activation='softmax', name='auxilliary_output_2')(x2)

x = inception_module(x,
                     filters_1x1=256,
                     filters_3x3_reduce=160,
                     filters_3x3=320,
                     filters_5x5_reduce=32,
                     filters_5x5=128,
                     filters_pool_proj=128,
                     name='inception_4e')

x = MaxPooling1D((3), padding='same', strides=2, name='max_pool_4_3x3/2')(x)

x = inception_module(x,
                     filters_1x1=256,
                     filters_3x3_reduce=160,
                     filters_3x3=320,
                     filters_5x5_reduce=32,
                     filters_5x5=128,
                     filters_pool_proj=128,
                     name='inception_5a')

x = inception_module(x,
                     filters_1x1=384,
                     filters_3x3_reduce=192,
                     filters_3x3=384,
                     filters_5x5_reduce=48,
                     filters_5x5=128,
                     filters_pool_proj=128,
                     name='inception_5b')

x = GlobalAveragePooling1D(name='avg_pool_5_3x3/1')(x)

x = Dropout(0.4)(x)
x = Dense(3, activation='softmax', name='output')(x)

Google_Net= keras.models.Model(inputs=[input_B], outputs=[x,x1,x2])

from tensorflow.keras import callbacks
from tensorflow.keras.callbacks import EarlyStopping
import keras

import tensorflow as tf
early_stopping_cb =  tf.keras.callbacks.EarlyStopping(patience=500,
restore_best_weights=True)
lr_scheduler1 = tf.keras.callbacks.ReduceLROnPlateau(factor=0.333, patience=50)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
from tensorflow.keras import optimizers

Google_Net= keras.models.load_model("/Saved wight/coran_H1N1_no_after_18.h5")

import tensorflow as tf
import os,datetime
k=10
checkpoint_cb = keras.callbacks.ModelCheckpoint("coran_H1N1_no.h5",
save_best_only=True)
all_scores = []
all_mse=[]
all_mae_histories = []
num_val_samples = len(train_set) // k
for i in range(k):
  print('processing fold #', i)
  val_data = train_set[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = target_train[i * num_val_samples: (i + 1) * num_val_samples]
  partial_train_data = np.concatenate(
                      [train_set[:i * num_val_samples],
                      train_set[(i + 1) * num_val_samples:]],
                      axis=0)
  partial_train_targets = np.concatenate(
                          [target_train[:i * num_val_samples],
                          target_train[(i + 1) * num_val_samples:]],
                          axis=0)
  from keras import losses
  optimizer = optimizers.Adamax(lr=0.0001 ,beta_1=0.99, beta_2=0.9)
  Google_Net.compile(loss=['mean_squared_error', 'mean_squared_error', 'mean_squared_error'], loss_weights=[0.3, 0.3, 0.3], optimizer=optimizer, metrics=['accuracy'])
  logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

  history1 = Google_Net.fit(partial_train_data, [partial_train_targets, partial_train_targets, partial_train_targets], validation_data=(val_data, [val_targets, val_targets, val_targets]),
                            epochs=1000, steps_per_epoch=2,validation_steps=2,batch_size=32,validation_batch_size=32,callbacks=[tensorboard_callback,early_stopping_cb,lr_scheduler1,checkpoint_cb])
  Google_Net.save("coran_H1N1_no_after.h5")





import pandas as pd
import matplotlib.pyplot as plt
pd.DataFrame(history1.history).plot(figsize=(12, 12))
plt.grid(True)
plt.gca().set_xlim(0, 200)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.xlabel('number of epochs', fontsize=14)
plt.ylabel('accuracy and loss', fontsize=14)
plt.title('cnn_model change in medication prediction with Google Net ',fontsize=16)
plt.show()

y_score_model1,y_score_model2,y_score_model3=Google_Net.predict(test_set,verbose=2)

y_score_model3

Google_Net.evaluate(test_set,target_test)

model = keras.models.load_model("/content/coran_H1N1_no_after.h5")

model.evaluate(test_set,target_test)

from sklearn.metrics import confusion_matrix
y_test1 =np.argmax(y_score_model1, axis=1)
real_test1 =np.argmax(target_test, axis=1)
cm1 = confusion_matrix(real_test1,y_test1)
print('Confusion Matrix : \n', cm1)

total1=sum(sum(cm1))
#####from confusion matrix calculate accuracy
accuracy1=(cm1[0,0]+cm1[1,1]+cm1[2,2])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1]+cm1[0,2])
sensitivity2 = cm1[1,1]/(cm1[1,0]+cm1[1,1]+cm1[1,2])
sensitivity3 = cm1[2,2]/(cm1[2,0]+cm1[2,1]+cm1[2,2])
print('Sensitivity of class NO: ', sensitivity1 )
print('Sensitivity of class H1N1: ', sensitivity2 )
print('Sensitivity of class COVID-19: ', sensitivity3 )
specificity1 = cm1[0,0]/(cm1[0,0]+cm1[1,0]+cm1[2,0])
specificity2 = cm1[1,1]/(cm1[0,1]+cm1[1,1]+cm1[2,1])
specificity3 = cm1[2,2]/(cm1[0,2]+cm1[1,2]+cm1[2,2])
print('Specificity of class NO: ', specificity1)
print('Specificity of class H1N1: ', specificity2)
print('Specificity of class COVID-19: ', specificity3)

!pip install scikit-plot

import scikitplot as skplt
import matplotlib.pyplot as plt

import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (5,5)
#y_pred = np.argmax(Y_pred, axis=1)
#y_test1 =np.argmax(y_test, axis=1)
skplt.metrics.plot_roc_curve(real_test1,y_score_model3,title="ROC Curves with proposed Classifier", cmap='BuGn', figsize=[5,5])
plt.show()



skplt.metrics.plot_confusion_matrix(real_test1, y_test1, normalize=True,cmap='copper_r',figsize=[5,5])

model = Google_Net

#!cp /content/coran_H1N1_no_after.h5 /content/drive/MyDrive

#!cp /content/coran_H1N1_no.h5 /content/drive/MyDrive
